{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qJ-TdQBt3mg6"},"outputs":[],"source":["# Set to false if you are not running\n","# this notebook in Google Colaboratory\n","run_on_colab = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23759,"status":"ok","timestamp":1740406321327,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"},"user_tz":-120},"id":"ZI-LIbZo5AYY","outputId":"ecf14771-70a3-4c9e-ec13-acfe37dd76b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if(run_on_colab):\n","  from google.colab import drive\n","  # This will prompt for authorization.\n","  drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3775,"status":"ok","timestamp":1740406329736,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"},"user_tz":-120},"id":"Mrn6NePm5E44","outputId":"7b49d64f-34d6-4435-cff9-1c6d4d1839cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: music21 in /usr/local/lib/python3.11/dist-packages (9.3.0)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from music21) (5.2.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from music21) (1.4.2)\n","Requirement already satisfied: jsonpickle in /usr/local/lib/python3.11/dist-packages (from music21) (4.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from music21) (3.10.0)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from music21) (10.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from music21) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from music21) (2.32.3)\n","Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.11/dist-packages (from music21) (24.11.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2025.1.31)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\n"]}],"source":["!pip install music21;"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":554758,"status":"ok","timestamp":1739698520424,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"},"user_tz":-120},"id":"tTvRIm0J5Kl_","outputId":"a463a732-49af-4475-bfac-f30fb177587f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-8d12e2b2-cc14-4263-a83c-32448ee5e743\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-8d12e2b2-cc14-4263-a83c-32448ee5e743\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving finalzip.zip to finalzip.zip\n"]}],"source":["if(run_on_colab):\n","  from google.colab import files\n","  files.upload()\n"]},{"cell_type":"markdown","source":["This script processes MIDI files from a specified folder using the music21 library. For each file, it extracts the average tempo, key, time signature, and note/chord/rest data, then stores the results in a list. Finally, it pickles the collected features for later use and prints out a brief sample for verification"],"metadata":{"id":"wJC8utej0Rsr"}},{"cell_type":"code","source":["import os\n","import music21\n","import pickle\n","from music21 import converter, note, chord, tempo, meter\n","\n","# path\n","midi_path = os.path.join(\"extracted_files\", \"finalzip\")\n","features = []\n","\n","for file in os.listdir(midi_path):\n","    if file.lower().endswith((\".mid\", \".midi\")):\n","        try:\n","            midi_data = converter.parse(os.path.join(midi_path, file))\n","\n","            # 1) extract temp\n","            tempos = midi_data.flat.getElementsByClass(tempo.MetronomeMark)\n","            if tempos:\n","                avg_tempo = sum(t.number for t in tempos) / len(tempos)\n","            else:\n","                avg_tempo = 120.0\n","\n","            # 2) extrat key\n","            key_signature = midi_data.analyze('key')\n","            key_name = key_signature.name\n","\n","            # 3) extract time signature\n","            time_sigs = midi_data.flat.getElementsByClass(meter.TimeSignature)\n","            if time_sigs:\n","                time_sig = time_sigs[0].ratioString  #for exemple : '4/4'\n","            else:\n","                time_sig = \"4/4\"\n","\n","            # 4) Extract (Note, Chord, Rest)\n","            notes_data = []\n","            for elem in midi_data.flat.notesAndRests:\n","                offset = elem.offset\n","                duration = elem.quarterLength\n","\n","                if isinstance(elem, note.Note):\n","                    notes_data.append({\n","                        \"type\": \"note\",\n","                        \"pitch\": str(elem.pitch),\n","                        \"offset\": offset,\n","                        \"duration\": duration\n","                    })\n","                elif isinstance(elem, chord.Chord):\n","                    # pitch classes or normalOrder\n","                    chord_str = '.'.join(str(n) for n in elem.normalOrder)\n","                    notes_data.append({\n","                        \"type\": \"chord\",\n","                        \"chord\": chord_str,\n","                        \"offset\": offset,\n","                        \"duration\": duration\n","                    })\n","                elif isinstance(elem, note.Rest):\n","                    notes_data.append({\n","                        \"type\": \"rest\",\n","                        \"offset\": offset,\n","                        \"duration\": duration\n","                    })\n","                # If you would like to address other elements, you can add them here.\n","\n","            # 5) save the data\n","            features.append({\n","                \"file\": file,\n","                \"tempo\": avg_tempo,\n","                \"key\": key_name,\n","                \"time_signature\": time_sig,\n","                \"notes\": notes_data\n","            })\n","\n","            print(f\"Processed {file}\")\n","\n","        except Exception as e:\n","            print(f\"Error processing {file}: {e}\")\n","\n","# save to pkl file\n","file_path = '/content/drive/My Drive/extracted_features.pkl'\n","with open(file_path, 'wb') as f:\n","    pickle.dump(features, f)\n","\n","#print for check\n","for feature in features[:5]:\n","    print(feature)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"efGpC_yPEIw9","executionInfo":{"status":"error","timestamp":1741526732518,"user_tz":-120,"elapsed":2893,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"b3fc1656-c0b6-4b74-d3db-65af1ea9ca76"},"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'extracted_files/finalzip'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1eae85f98760>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".mid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".midi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extracted_files/finalzip'"]}]},{"cell_type":"markdown","source":["This script loads previously extracted MIDI features, then creates a token sequence for each file by:\n","\n","1. Sorting events by offset.\n","2. Converting each note, chord, or rest to a discrete token that includes its pitch/chord (if applicable) and quantized duration.\n","3. Adding context tokens for tempo, key, and time signature.\n","The final token sequences are saved to a pickle file for later use.\n"],"metadata":{"id":"UWSn5kGa0IXE"}},{"cell_type":"code","source":["import math\n","from fractions import Fraction\n","import pickle\n","import os\n","\n","def quantize_to_16th(value):\n","    \"\"\"\n","   Converts value into quarters.\n","    \"\"\"\n","    return int(round(float(value) * 4))\n","\n","def event_to_token(event):\n","    \"\"\"\n","This function converts a musical event (represented as a dictionary) into a token string. Each event has a \"type\" (which can be \"note\", \"chord\", or \"rest\") and a \"duration\". If the type is \"note\", there's also a \"pitch\" key; if it's \"chord\", there's a \"chord\" key (a series of numbers separated by dots). The output token string is formatted as:\n","\n","For a note: NOTE_<pitch>_<duration> (e.g., NOTE_C4_4)\n","For a chord: CHORD_<chord>_<duration> (e.g., CHORD_1.6_6)\n","For a rest: REST_<duration> (e.g., REST_2)\n","    \"\"\"\n","    event_type = event[\"type\"]\n","    dur_quant = quantize_to_16th(event[\"duration\"])\n","\n","    if event_type == \"note\":\n","        pitch_str = event[\"pitch\"]\n","        return f\"NOTE_{pitch_str}_{dur_quant}\"\n","    elif event_type == \"chord\":\n","        chord_str = event[\"chord\"]\n","        return f\"CHORD_{chord_str}_{dur_quant}\"\n","    elif event_type == \"rest\":\n","        return f\"REST_{dur_quant}\"\n","    else:\n","        return \"UNK\"\n","\n","def build_token_sequence(notes_data):\n","    \"\"\"\n","\n","    This step sorts the list of events by their \"offset\" property and then converts each event into a token string using the defined format.\n","    \"\"\"\n","    # sort by offset\n","    notes_sorted = sorted(notes_data, key=lambda x: float(x[\"offset\"]))\n","    token_seq = [event_to_token(event) for event in notes_sorted]\n","    return token_seq\n","\n","# load the data\n","features_path = '/content/drive/My Drive/extracted_features.pkl'\n","with open(features_path, 'rb') as f:\n","    features = pickle.load(f)\n","\n","# for each sequence token\n","all_token_sequences = []\n","for feat in features:\n","    notes_data = feat[\"notes\"]\n","    token_seq = build_token_sequence(notes_data)\n","\n","\n","    tempo_token = f\"TEMPO_{round(feat['tempo'],2)}\"\n","    key_token = f\"KEY_{feat['key'].replace(' ', '_')}\"\n","    time_sig_token = f\"TIME_{feat['time_signature'].replace('/', '_')}\"\n","\n","    full_seq = [tempo_token, key_token, time_sig_token] + token_seq\n","    all_token_sequences.append(full_seq)\n","\n","# statistics & checks\n","print(\"Total number of files:\", len(all_token_sequences))\n","print(\"Sample token sequence (first 20 tokens) from first file:\")\n","print(all_token_sequences[0][:20])\n","\n","# save\n","output_path = '/content/drive/My Drive/all_token_sequences.pkl'\n","with open(output_path, 'wb') as f:\n","    pickle.dump(all_token_sequences, f)\n","\n","print(\"âœ… Token sequences saved to:\", output_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyEkF3JNQz8K","executionInfo":{"status":"ok","timestamp":1739774445669,"user_tz":-120,"elapsed":16133,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"9d04fb19-83d0-44a6-a970-88e60a840b38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of files: 830\n","Sample token sequence (first 20 tokens) from first file:\n","['TEMPO_120.0', 'KEY_F#_major', 'TIME_4_4', 'REST_8', 'REST_8', 'REST_10', 'CHORD_1.6_5', 'NOTE_B-4_8', 'NOTE_B-3_1', 'NOTE_C#4_1', 'CHORD_1.6_1', 'CHORD_10.1_1', 'REST_3', 'REST_0', 'CHORD_10.1_1', 'CHORD_1.6_2', 'REST_4', 'REST_8', 'REST_14', 'NOTE_B-4_1']\n","âœ… Token sequences saved to: /content/drive/My Drive/all_token_sequences.pkl\n"]}]},{"cell_type":"markdown","source":["This script loads preprocessed token sequences, builds a vocabulary (mapping tokens to integers), and generates training data using a sliding window (length 200) to create input-target pairs. Finally, it saves the training arrays and the mapping dictionaries as pickle files for future use.\n"],"metadata":{"id":"8-NnRxj8WN3E"}},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import random\n","\n","\n","with open('/content/drive/My Drive/all_token_sequences.pkl', 'rb') as f:\n","    all_token_sequences = pickle.load(f)\n","\n","print(\"Total token sequences (files):\", len(all_token_sequences))\n","\n","# build vocabulary\n","vocab = set()\n","for seq in all_token_sequences:\n","    vocab.update(seq)\n","vocab = sorted(list(vocab))\n","note_to_int = {token: i for i, token in enumerate(vocab)}\n","int_to_note = {i: token for i, token in enumerate(vocab)}\n","n_vocab = len(vocab)\n","\n","print(\"Vocabulary size:\", n_vocab)\n","print(\"Sample vocabulary tokens:\", vocab[:20])\n","\n","# Setting a training sequence length\n","sequence_length = 200\n","\n","# Creating Input and Target Lists\n","# For each file sequence, we use a sliding window that generates:\n","#   input: [t0, t1, ..., t_{L-1}]\n","#   target: [t1, t2, ..., t_L]\n","train_input = []\n","train_target = []\n","for seq in all_token_sequences:\n","\n","    if len(seq) < sequence_length + 1:\n","        continue\n","\n","    for i in range(0, len(seq) - sequence_length):\n","        input_seq = seq[i : i + sequence_length]\n","        target_seq = seq[i + 1 : i + sequence_length + 1]\n","        train_input.append([note_to_int[token] for token in input_seq])\n","        train_target.append([note_to_int[token] for token in target_seq])\n","\n","train_input = np.array(train_input)\n","train_target = np.array(train_target)\n","\n","print(\"Training data shapes:\")\n","print(\"Input:\", train_input.shape)\n","print(\"Target:\", train_target.shape)\n","\n","# save data\n","with open('/content/drive/My Drive/training_input.pkl', 'wb') as f:\n","    pickle.dump(train_input, f)\n","with open('/content/drive/My Drive/training_target.pkl', 'wb') as f:\n","    pickle.dump(train_target, f)\n","with open('/content/drive/My Drive/note_to_int.pkl', 'wb') as f:\n","    pickle.dump(note_to_int, f)\n","with open('/content/drive/My Drive/int_to_note.pkl', 'wb') as f:\n","    pickle.dump(int_to_note, f)\n","\n","print(\"âœ… Training dataset saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"dDrQkw5QRh1K","executionInfo":{"status":"error","timestamp":1740406479556,"user_tz":-120,"elapsed":24698,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"38c665a3-1460-41e2-e4ac-449e9596c144"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total token sequences (files): 830\n","Vocabulary size: 11050\n","Sample vocabulary tokens: ['CHORD_0.1.2.3.4.5.6.7.8.9.10.11_1', 'CHORD_0.1.2.3.4.5.6.7.8.9.10_1', 'CHORD_0.1.2.3.4.5.6.8.9.10_1', 'CHORD_0.1.2.3.4.5.6_1', 'CHORD_0.1.2.3.4.5.8_1', 'CHORD_0.1.2.3.4.5_1', 'CHORD_0.1.2.3.4.6.7_1', 'CHORD_0.1.2.3.4.6_1', 'CHORD_0.1.2.3.4.7.8_1', 'CHORD_0.1.2.3.4.7_1', 'CHORD_0.1.2.3.4.8_1', 'CHORD_0.1.2.3.4_1', 'CHORD_0.1.2.3.5.6.8_1', 'CHORD_0.1.2.3.5.6.9_1', 'CHORD_0.1.2.3.5.6_1', 'CHORD_0.1.2.3.5.7_1', 'CHORD_0.1.2.3.5.8.9_1', 'CHORD_0.1.2.3.5_1', 'CHORD_0.1.2.3.6.7_1', 'CHORD_0.1.2.3.6.8.9_1']\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3f8d84d6e239>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-3f8d84d6e239>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnote_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["load the files"],"metadata":{"id":"lbqEIaZEUKSX"}},{"cell_type":"code","source":["import pickle\n","\n","# Load processed token sequences\n","with open('/content/drive/My Drive/all_token_sequences.pkl', 'rb') as f:\n","    all_token_sequences = pickle.load(f)\n","\n","# Load training inputs\n","with open('/content/drive/My Drive/training_input.pkl', 'rb') as f:\n","    train_input = pickle.load(f)\n","\n","# Load training targets\n","with open('/content/drive/My Drive/training_target.pkl', 'rb') as f:\n","    train_target = pickle.load(f)\n","\n","# Load token-to-integer mapping\n","with open('/content/drive/My Drive/note_to_int.pkl', 'rb') as f:\n","    note_to_int = pickle.load(f)\n","\n","# Load integer-to-token mapping\n","with open('/content/drive/My Drive/int_to_note.pkl', 'rb') as f:\n","    int_to_note = pickle.load(f)\n","\n","print(\"âœ… All files loaded successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EusSCIyzUKDD","executionInfo":{"status":"ok","timestamp":1740406816120,"user_tz":-120,"elapsed":237752,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"c601d9ba-6dd9-4a4b-f134-d5da31a3579e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All files loaded successfully.\n"]}]},{"cell_type":"markdown","source":["This code defines a transformer-based music generator in PyTorch. It includes:\n","\n","- An embedding layer to convert input tokens into vectors.\n","- A precomputed positional encoding (using sine and cosine functions) to inject sequence information.\n","- A transformer encoder to process the sequence with multiple layers and attention heads.\n","- A final linear layer that projects the transformer output to the vocabulary size, producing logits for each token position."],"metadata":{"id":"-P4H5CgFWcou"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class MusicGenerator(nn.Module):\n","    def __init__(self, n_vocab, sequence_length, embed_dim=256, num_heads=8, num_layers=4):\n","        super(MusicGenerator, self).__init__()\n","        # Embedding layer: converts token indices into embedding vectors\n","        self.embedding = nn.Embedding(n_vocab, embed_dim)\n","        # Register positional encoding buffer: generates positional encodings for 2 * sequence_length tokens\n","        self.register_buffer(\"positional_encoding\", self._generate_positional_encoding(sequence_length * 2, embed_dim))\n","        # Transformer encoder: composed of several transformer encoder layers\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(\n","                d_model=embed_dim,\n","                nhead=num_heads,\n","                dim_feedforward=512,\n","                dropout=0.3,\n","                batch_first=True,\n","                norm_first=True\n","            ),\n","            num_layers=num_layers\n","        )\n","        # Fully connected layer: projects transformer outputs to vocabulary size logits\n","        self.fc = nn.Linear(embed_dim, n_vocab)  # Output dimension equals the vocabulary size\n","\n","    def _generate_positional_encoding(self, seq_len, embed_dim):\n","        \"\"\"\n","        Generates positional encoding using sine and cosine functions.\n","        Returns a tensor of shape (1, seq_len, embed_dim) to be added to the embeddings.\n","        \"\"\"\n","        # Create a tensor of positions from 0 to seq_len - 1\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","        # Calculate the div_term for the exponential decay factor\n","        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n","        pos_enc = torch.zeros(seq_len, embed_dim)\n","        # Apply sine to even indices of the embedding dimension\n","        pos_enc[:, 0::2] = torch.sin(position * div_term)\n","        # Apply cosine to odd indices of the embedding dimension\n","        pos_enc[:, 1::2] = torch.cos(position * div_term)\n","        return pos_enc.unsqueeze(0)  # Add a batch dimension\n","\n","    def forward(self, x):\n","        # Convert input token indices to embedding vectors: shape [B, sequence_length, embed_dim]\n","        embedded = self.embedding(x)\n","        seq_len = x.size(1)\n","        # Extract the corresponding positional encoding for the current sequence length\n","        pos_enc = self.positional_encoding[:, :seq_len, :]\n","        # Add positional encoding to the embeddings\n","        x = embedded + pos_enc\n","        # Pass the input through the transformer encoder\n","        x = self.transformer(x)\n","        # Project the transformer output to logits for each token in the vocabulary\n","        x = self.fc(x)  # Output shape: [B, sequence_length, n_vocab]\n","        return x\n"],"metadata":{"id":"fZ8AaMyEyvTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This class implements a music discriminator network. It embeds input token sequences, flattens them, and processes the result through several fully connected layers with LeakyReLU and dropout, ending with a sigmoid output that indicates the probability of the input being real.\n"],"metadata":{"id":"9PUqKUGCWw8V"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MusicDiscriminator(nn.Module):\n","    def __init__(self, n_vocab, sequence_length, embed_dim=256):\n","        super(MusicDiscriminator, self).__init__()\n","        # Use an embedding layer to convert input tokens into embedding vectors.\n","        self.embedding = nn.Embedding(n_vocab, embed_dim)\n","        # A fully connected network to classify the sequence as real or fake.\n","        self.fc = nn.Sequential(\n","            nn.Linear(sequence_length * embed_dim, 1024),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        # Convert input token indices into embedding vectors.\n","        # Output shape: [B, sequence_length, embed_dim]\n","        x = self.embedding(x)\n","        # Flatten the embeddings into a single vector per batch element.\n","        # Output shape: [B, sequence_length * embed_dim]\n","        x = x.view(x.size(0), -1)\n","        # Pass the flattened vector through the fully connected layers.\n","        # Output shape: [B, 1]\n","        x = self.fc(x)\n","        return x\n"],"metadata":{"id":"vSaHC_sTy4H5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This snippet initializes the generator and discriminator models using the previously obtained vocabulary size (n_vocab) and sequence length (e.g., 200). It then transfers both models to the available device (GPU if available, otherwise CPU) and prints their architectures for verification."],"metadata":{"id":"6Kq77zQlXAN6"}},{"cell_type":"code","source":["# Model initialization â€“ we use the provided variable n_vocab and the sequence length from train_inputs.shape[1] (for example, 200)\n","generator = MusicGenerator(\n","    n_vocab=n_vocab,\n","    sequence_length=train_input.shape[1],\n","    embed_dim=128,\n","    num_heads=4,\n","    num_layers=3\n",")\n","\n","discriminator = MusicDiscriminator(\n","    n_vocab=n_vocab,\n","    sequence_length=train_input.shape[1],\n","    embed_dim=128\n",")\n","\n","# Moving to the appropriate device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","\n","print(\"Generator:\")\n","print(generator)\n","print(\"Discriminator:\")\n","print(discriminator)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rROrNthWy63m","executionInfo":{"status":"ok","timestamp":1739775261091,"user_tz":-120,"elapsed":1221,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"25de7a4e-0080-49f5-a83a-81b629bb6bfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Generator:\n","MusicGenerator(\n","  (embedding): Embedding(11050, 128)\n","  (transformer): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=512, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (fc): Linear(in_features=128, out_features=11050, bias=True)\n",")\n","Discriminator:\n","MusicDiscriminator(\n","  (embedding): Embedding(11050, 128)\n","  (fc): Sequential(\n","    (0): Linear(in_features=25600, out_features=1024, bias=True)\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): Linear(in_features=1024, out_features=512, bias=True)\n","    (4): LeakyReLU(negative_slope=0.2)\n","    (5): Dropout(p=0.3, inplace=False)\n","    (6): Linear(in_features=512, out_features=256, bias=True)\n","    (7): LeakyReLU(negative_slope=0.2)\n","    (8): Dropout(p=0.3, inplace=False)\n","    (9): Linear(in_features=256, out_features=1, bias=True)\n","    (10): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["This code sets up the training environment by:\n","\n","- Creating a directory for saving checkpoints and defining training parameters (epochs, batch size, and save frequency).\n","- Loading training input and target data from pickle files and constructing a DataLoader.\n","- Moving pre-initialized generator and discriminator models to the appropriate device (GPU if available).\n","- Initializing loss functions (BCELoss and CrossEntropyLoss) and Adam optimizers for both models.\n","- Preparing a variable to track the best generator loss during training."],"metadata":{"id":"vzz72cVkXK3-"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from tqdm import tqdm\n","import pickle\n","\n","# Basic configurations\n","save_path = \"/content/drive/My Drive/checkpoints/\"\n","os.makedirs(save_path, exist_ok=True)\n","\n","epochs = 50  # You can start with 50-100 epochs\n","batch_size = 512\n","save_every = 5  # Save every 5 epochs\n","\n","# Move to the appropriate device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the training data (if not already loaded)\n","with open('/content/drive/My Drive/training_input.pkl', 'rb') as f:\n","    train_inputs = pickle.load(f)\n","with open('/content/drive/My Drive/training_target.pkl', 'rb') as f:\n","    train_targets = pickle.load(f)\n","\n","print(\"Training inputs shape:\", train_inputs.shape)\n","print(\"Training targets shape:\", train_targets.shape)\n","\n","# Create DataLoader\n","dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.long),\n","                        torch.tensor(train_targets, dtype=torch.long))\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Assume that the models (generator, discriminator) have been defined and initialized as done previously\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","\n","# Define loss functions and optimizers\n","bce_loss = nn.BCELoss()\n","ce_loss = nn.CrossEntropyLoss()\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001)\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n","\n","best_g_loss = float('inf')\n"],"metadata":{"id":"siEuO5JcXGur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This training loop updates the generator and discriminator in a GAN setup over multiple epochs. For each batch:\n","\n","- Discriminator Update:\n"," - Real sequences (targets) and fake sequences (generated from inputs) are evaluated.\n"," - Binary cross-entropy losses are computed for both, and their average is used to update the discriminator.\n","- Generator Update:\n"," - The generator produces fake sequences which are compared to targets using cross-entropy loss (teacher forcing).\n"," - An adversarial loss (to fool the discriminator) is also computed and scaled.\n"," - The combined loss updates the generator.\n","\n","Epoch losses are averaged, printed, and model checkpoints are saved when improvements occur or at regular intervals."],"metadata":{"id":"hp65SF8MXa4L"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","    generator.train()\n","    discriminator.train()\n","\n","    epoch_g_loss = 0.0\n","    epoch_d_loss = 0.0\n","\n","    for batch_input, batch_target in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","        batch_input = batch_input.to(device)\n","        batch_target = batch_target.to(device).long()\n","\n","        ## === Update Discriminator === ##\n","        optimizer_D.zero_grad()\n","        real_labels = torch.ones(batch_input.shape[0], 1).to(device)\n","        fake_labels = torch.zeros(batch_input.shape[0], 1).to(device)\n","\n","        # For real data: use the discriminator (which includes an embedding) on the target indices\n","        real_validity = discriminator(batch_target)  # [B, 1]\n","        loss_real = bce_loss(real_validity, real_labels)\n","\n","        # For fake data:\n","        fake_logits = generator(batch_input)  # [B, sequence_length, n_vocab]\n","        fake_tokens = torch.argmax(fake_logits, dim=-1).long()  # [B, sequence_length]\n","        fake_validity = discriminator(fake_tokens.detach())  # [B, 1]\n","        loss_fake = bce_loss(fake_validity, fake_labels)\n","\n","        d_loss = (loss_real + loss_fake) / 2.0\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        ## === Update Generator === ##\n","        optimizer_G.zero_grad()\n","\n","        fake_logits = generator(batch_input)  # [B, sequence_length, n_vocab]\n","        # Teacher Forcing Loss: compute loss against the target\n","        g_teacher_loss = ce_loss(fake_logits.view(-1, n_vocab), batch_target.view(-1))\n","\n","        # Adversarial Loss: encourage the discriminator to believe that fake sequences are real\n","        fake_tokens = torch.argmax(fake_logits, dim=-1).long()\n","        validity = discriminator(fake_tokens.detach())\n","        g_adv_loss = bce_loss(validity, real_labels)\n","\n","        g_loss = g_teacher_loss + 0.002 * g_adv_loss\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        epoch_g_loss += g_loss.item()\n","        epoch_d_loss += d_loss.item()\n","\n","    avg_g_loss = epoch_g_loss / len(data_loader)\n","    avg_d_loss = epoch_d_loss / len(data_loader)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Generator Loss: {avg_g_loss:.4f}, Discriminator Loss: {avg_d_loss:.4f}\")\n","\n","    # Save the model if it has improved\n","    if avg_g_loss < best_g_loss:\n","        best_g_loss = avg_g_loss\n","        save_file = os.path.join(save_path, f\"best_model_Gloss_{avg_g_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, save_file)\n","        print(f\"âœ… Model saved at epoch {epoch+1} (Best Generator Loss: {best_g_loss:.4f})\")\n","\n","    # Save the model every X epochs\n","    if (epoch + 1) % save_every == 0:\n","        save_file = os.path.join(save_path, f\"epoch_{epoch+1}_Gloss_{avg_g_loss:.4f}_Dloss_{avg_d_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, save_file)\n","        print(f\"ðŸ’¾ Saved model at epoch {epoch+1}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"D6noYPJM0Pmr","executionInfo":{"status":"error","timestamp":1739775193783,"user_tz":-120,"elapsed":413295,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"75415c4b-4478-4987-8c86-2c673fcac318"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training inputs shape: (3510447, 200)\n","Training targets shape: (3510447, 200)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50:  13%|â–ˆâ–Ž        | 923/6857 [06:36<42:31,  2.33it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-765ead6464ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mepoch_g_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mepoch_d_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["This snippet defines a scaled-down version of our model architecture. Due to hardware limitations, we reduced the model's sizeâ€”using a smaller embedding dimension, fewer layers, and other optimizationsâ€”to ensure efficient training and inference while still meeting the project's core\n"],"metadata":{"id":"J__6HYxgTYwh"}},{"cell_type":"code","source":["import os\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","from tqdm import tqdm\n","import pickle\n","\n","###########################\n","# Definition of the small models\n","###########################\n","\n","class SmallMusicGenerator(nn.Module):\n","    def __init__(self, n_vocab, sequence_length, embed_dim=128, num_heads=4, num_layers=3):\n","        super(SmallMusicGenerator, self).__init__()\n","        self.embedding = nn.Embedding(n_vocab, embed_dim)\n","        # Build positional encoding for double the input length\n","        self.register_buffer(\"positional_encoding\", self._generate_positional_encoding(sequence_length * 2, embed_dim))\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(\n","                d_model=embed_dim,\n","                nhead=num_heads,\n","                dim_feedforward=256,  # Reduced dimension for the FFN\n","                dropout=0.3,\n","                batch_first=True,\n","                norm_first=True\n","            ),\n","            num_layers=num_layers\n","        )\n","        self.fc = nn.Linear(embed_dim, n_vocab)\n","\n","    def _generate_positional_encoding(self, seq_len, embed_dim):\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n","        pos_enc = torch.zeros(seq_len, embed_dim)\n","        pos_enc[:, 0::2] = torch.sin(position * div_term)\n","        pos_enc[:, 1::2] = torch.cos(position * div_term)\n","        return pos_enc.unsqueeze(0)  # Add a batch dimension\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)  # [B, sequence_length, embed_dim]\n","        seq_len = x.size(1)\n","        pos_enc = self.positional_encoding[:, :seq_len, :]\n","        x = embedded + pos_enc\n","        x = self.transformer(x)\n","        x = self.fc(x)  # [B, sequence_length, n_vocab]\n","        return x\n","\n","class SmallMusicDiscriminator(nn.Module):\n","    def __init__(self, n_vocab, sequence_length, embed_dim=128):\n","        super(SmallMusicDiscriminator, self).__init__()\n","        self.embedding = nn.Embedding(n_vocab, embed_dim)\n","        self.fc = nn.Sequential(\n","            nn.Linear(sequence_length * embed_dim, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, 1)  # No Sigmoid applied because we use BCEWithLogitsLoss\n","        )\n","\n","    def forward(self, x):\n","        x = self.embedding(x)  # [B, sequence_length, embed_dim]\n","        x = x.view(x.size(0), -1)  # Flatten\n","        x = self.fc(x)  # [B, 1]\n","        return x\n"],"metadata":{"id":"zUmpLXsU21YG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####################################\n","# Model Initialization\n","####################################\n","# Assume that note_to_int has already been loaded, and compute n_vocab\n","# with open('/content/drive/My Drive/note_to_int.pkl', 'rb') as f:\n","#     note_to_int = pickle.load(f)\n","# n_vocab = len(note_to_int)\n","\n","# Assume that train_inputs has already been loaded (for example, from the saved training data)\n","# with open('/content/drive/My Drive/training_input.pkl', 'rb') as f:\n","#     train_inputs = pickle.load(f)\n","# with open('/content/drive/My Drive/training_target.pkl', 'rb') as f:\n","#     train_targets = pickle.load(f)\n","\n","print(\"Training inputs shape:\", train_inputs.shape)\n","print(\"Training targets shape:\", train_targets.shape)\n","\n","# Initialize the models\n","sequence_length = train_inputs.shape[1]  # should be 200\n","generator = SmallMusicGenerator(n_vocab=n_vocab, sequence_length=sequence_length)\n","discriminator = SmallMusicDiscriminator(n_vocab=n_vocab, sequence_length=sequence_length)\n","\n","# Move models to the appropriate device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","generator.positional_encoding = generator.positional_encoding.to(device)\n","\n","print(\"Small Generator:\")\n","print(generator)\n","print(\"Small Discriminator:\")\n","print(discriminator)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"o8AjeZSRWGHO","executionInfo":{"status":"error","timestamp":1740407080670,"user_tz":-120,"elapsed":364,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"64afb857-9ae1-4124-8f88-b3417095f7bd"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_inputs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-aef5c2770de7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     train_targets = pickle.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training inputs shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training targets shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"]}]},{"cell_type":"markdown","source":["Due to hardware constraints, we also implemented a smaller model variant and trained it accordingly. The training procedure mirrors the full model's loopâ€”with periodic checkpointing and loss monitoringâ€”but uses reduced parameters and layers to decrease computational load while still meeting the project's objectives."],"metadata":{"id":"9okWvj2rYN2x"}},{"cell_type":"code","source":["####################################\n","# Preparing Dataset and DataLoader\n","####################################\n","dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.long),\n","                        torch.tensor(train_targets, dtype=torch.long))\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","####################################\n","# Defining Loss Functions and Optimizers\n","####################################\n","# Use BCEWithLogitsLoss for the discriminator\n","bce_loss = nn.BCEWithLogitsLoss()\n","ce_loss = nn.CrossEntropyLoss()\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001)\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n","\n","# Set up AMP (Automatic Mixed Precision)\n","scaler = GradScaler()\n","\n","best_g_loss = float('inf')\n","\n","####################################\n","# Training Loop\n","####################################\n","for epoch in range(epochs):\n","    generator.train()\n","    discriminator.train()\n","\n","    epoch_g_loss = 0.0\n","    epoch_d_loss = 0.0\n","\n","    for batch_input, batch_target in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","        batch_input = batch_input.to(device).long()\n","        batch_target = batch_target.to(device).long()\n","\n","        ## === Updating Discriminator === ##\n","        optimizer_D.zero_grad()\n","        real_labels = torch.ones(batch_input.shape[0], 1).to(device)\n","        fake_labels = torch.zeros(batch_input.shape[0], 1).to(device)\n","\n","        with autocast():\n","            # For real data: pass batch_target (which is a sequence of indices)\n","            real_validity = discriminator(batch_target)  # [B, 1] (raw logits)\n","            loss_real = bce_loss(real_validity, real_labels)\n","\n","            # For fake data:\n","            fake_logits = generator(batch_input)  # [B, sequence_length, n_vocab]\n","            fake_tokens = torch.argmax(fake_logits, dim=-1).long()  # [B, sequence_length]\n","            fake_validity = discriminator(fake_tokens.detach())  # [B, 1]\n","            loss_fake = bce_loss(fake_validity, fake_labels)\n","\n","            d_loss = (loss_real + loss_fake) / 2.0\n","\n","        scaler.scale(d_loss).backward()\n","        scaler.step(optimizer_D)\n","        scaler.update()\n","\n","        ## === Updating Generator === ##\n","        optimizer_G.zero_grad()\n","        with autocast():\n","            fake_logits = generator(batch_input)  # [B, sequence_length, n_vocab]\n","            g_teacher_loss = ce_loss(fake_logits.view(-1, n_vocab), batch_target.view(-1))\n","\n","            fake_tokens = torch.argmax(fake_logits, dim=-1).long()\n","            validity = discriminator(fake_tokens.detach())\n","            g_adv_loss = bce_loss(validity, real_labels)\n","\n","            g_loss = g_teacher_loss + 0.002 * g_adv_loss\n","\n","        scaler.scale(g_loss).backward()\n","        scaler.step(optimizer_G)\n","        scaler.update()\n","\n","        epoch_g_loss += g_loss.item()\n","        epoch_d_loss += d_loss.item()\n","\n","    avg_g_loss = epoch_g_loss / len(data_loader)\n","    avg_d_loss = epoch_d_loss / len(data_loader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Generator Loss: {avg_g_loss:.4f}, Discriminator Loss: {avg_d_loss:.4f}\")\n","\n","    if avg_g_loss < best_g_loss:\n","        best_g_loss = avg_g_loss\n","        save_file = os.path.join(save_path, f\"best_model_Gloss_{avg_g_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, save_file)\n","        print(f\"âœ… Model saved at epoch {epoch+1} (Best Generator Loss: {best_g_loss:.4f})\")\n","\n","    if (epoch + 1) % save_every == 0:\n","        save_file = os.path.join(save_path, f\"epoch_{epoch+1}_Gloss_{avg_g_loss:.4f}_Dloss_{avg_d_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, save_file)\n","        print(f\"ðŸ’¾ Saved model at epoch {epoch+1}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Rl2x5paCr9Ps","executionInfo":{"status":"error","timestamp":1739787478266,"user_tz":-120,"elapsed":11935810,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"ee22eee6-d90d-4382-af3e-91563b918cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training inputs shape: (3510447, 200)\n","Training targets shape: (3510447, 200)\n","Small Generator:\n","SmallMusicGenerator(\n","  (embedding): Embedding(11050, 128)\n","  (transformer): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=256, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (fc): Linear(in_features=128, out_features=11050, bias=True)\n",")\n","Small Discriminator:\n","SmallMusicDiscriminator(\n","  (embedding): Embedding(11050, 128)\n","  (fc): Sequential(\n","    (0): Linear(in_features=25600, out_features=512, bias=True)\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): Linear(in_features=512, out_features=256, bias=True)\n","    (4): LeakyReLU(negative_slope=0.2)\n","    (5): Dropout(p=0.3, inplace=False)\n","    (6): Linear(in_features=256, out_features=1, bias=True)\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-6a1e06b8bf77>:121: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","Epoch 1/50:   0%|          | 0/6857 [00:00<?, ?it/s]<ipython-input-23-6a1e06b8bf77>:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","<ipython-input-23-6a1e06b8bf77>:163: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [11:01<00:00, 10.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Generator Loss: 2.2329, Discriminator Loss: 0.3424\n","âœ… Model saved at epoch 1 (Best Generator Loss: 2.2329)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:57<00:00, 10.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50, Generator Loss: 0.1594, Discriminator Loss: 0.4201\n","âœ… Model saved at epoch 2 (Best Generator Loss: 0.1594)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:58<00:00, 10.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50, Generator Loss: 0.1027, Discriminator Loss: 0.3669\n","âœ… Model saved at epoch 3 (Best Generator Loss: 0.1027)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:56<00:00, 10.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50, Generator Loss: 0.0844, Discriminator Loss: 0.3509\n","âœ… Model saved at epoch 4 (Best Generator Loss: 0.0844)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:56<00:00, 10.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50, Generator Loss: 0.0771, Discriminator Loss: 0.3426\n","âœ… Model saved at epoch 5 (Best Generator Loss: 0.0771)\n","ðŸ’¾ Saved model at epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:55<00:00, 10.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50, Generator Loss: 0.0718, Discriminator Loss: 0.3512\n","âœ… Model saved at epoch 6 (Best Generator Loss: 0.0718)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:58<00:00, 10.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50, Generator Loss: 0.0653, Discriminator Loss: 0.3562\n","âœ… Model saved at epoch 7 (Best Generator Loss: 0.0653)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [11:00<00:00, 10.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50, Generator Loss: 0.0539, Discriminator Loss: 0.3455\n","âœ… Model saved at epoch 8 (Best Generator Loss: 0.0539)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [11:00<00:00, 10.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50, Generator Loss: 0.0477, Discriminator Loss: 0.3255\n","âœ… Model saved at epoch 9 (Best Generator Loss: 0.0477)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:57<00:00, 10.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50, Generator Loss: 0.0453, Discriminator Loss: 0.3128\n","âœ… Model saved at epoch 10 (Best Generator Loss: 0.0453)\n","ðŸ’¾ Saved model at epoch 10\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:54<00:00, 10.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50, Generator Loss: 0.0439, Discriminator Loss: 0.3037\n","âœ… Model saved at epoch 11 (Best Generator Loss: 0.0439)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:56<00:00, 10.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50, Generator Loss: 0.0430, Discriminator Loss: 0.2962\n","âœ… Model saved at epoch 12 (Best Generator Loss: 0.0430)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:55<00:00, 10.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50, Generator Loss: 0.0424, Discriminator Loss: 0.2903\n","âœ… Model saved at epoch 13 (Best Generator Loss: 0.0424)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:59<00:00, 10.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50, Generator Loss: 0.0419, Discriminator Loss: 0.2845\n","âœ… Model saved at epoch 14 (Best Generator Loss: 0.0419)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:58<00:00, 10.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50, Generator Loss: 0.0416, Discriminator Loss: 0.2794\n","âœ… Model saved at epoch 15 (Best Generator Loss: 0.0416)\n","ðŸ’¾ Saved model at epoch 15\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [10:59<00:00, 10.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50, Generator Loss: 0.0413, Discriminator Loss: 0.2753\n","âœ… Model saved at epoch 16 (Best Generator Loss: 0.0413)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [11:02<00:00, 10.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50, Generator Loss: 0.0410, Discriminator Loss: 0.2714\n","âœ… Model saved at epoch 17 (Best Generator Loss: 0.0410)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [11:03<00:00, 10.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50, Generator Loss: 0.0408, Discriminator Loss: 0.2680\n","âœ… Model saved at epoch 18 (Best Generator Loss: 0.0408)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50:   9%|â–‰         | 641/6857 [01:01<10:00, 10.34it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-6a1e06b8bf77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["This snippet demonstrates how to load a saved checkpoint containing model weights and perform a quick test generation. It loads the checkpoint, restores the generator and discriminator states, and sets both models to evaluation mode. A dummy input sequence is then created and passed through the generator. The output logits are converted to probabilities, and the most likely token is chosen at each step. Finally, the predicted token indices are mapped back to their corresponding token strings, and a sample sequence is printed for inspection."],"metadata":{"id":"YdhjjCxhY1eB"}},{"cell_type":"code","source":["import torch\n","import pickle\n","from tqdm import tqdm\n","\n","# Define the checkpoint file path\n","checkpoint_path = \"/content/drive/My Drive/checkpoints/best_model_Gloss_0.0319.pth\"\n","\n","# Define the device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the checkpoint file\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","# Load the weights into the models (assuming generator and discriminator already exist)\n","generator.load_state_dict(checkpoint['generator_state_dict'])\n","discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","\n","# Move the models to the device and set them to evaluation mode\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","generator.eval()\n","discriminator.eval()\n","\n","print(f\"âœ… Checkpoint loaded from {checkpoint_path}\")\n","print(\"Generator in eval mode:\")\n","print(generator)\n","print(\"\\nDiscriminator in eval mode:\")\n","print(discriminator)\n","\n","# Perform an initial test:\n","# Create a random input from the training data array (if available) or use a dummy example.\n","# Assume that sequence_length is 200.\n","dummy_input = torch.randint(0, n_vocab, (1, 200), dtype=torch.long).to(device)\n","with torch.no_grad():\n","    output_logits = generator(dummy_input)  # Shape: [1, 200, n_vocab]\n","    output_probs = torch.softmax(output_logits, dim=-1)\n","    # Choose the token with the highest probability at each step\n","    predicted_tokens = torch.argmax(output_probs, dim=-1)\n","\n","    # Convert tokens to words using int_to_note\n","    predicted_sequence = [int_to_note[idx.item()] for idx in predicted_tokens[0]]\n","\n","print(\"\\nâœ… Sample generated sequence (first 50 tokens):\")\n","print(predicted_sequence[:50])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"Ww2Lrx11an-G","executionInfo":{"status":"error","timestamp":1740407042692,"user_tz":-120,"elapsed":12753,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"03a4b055-1635-447b-e60c-c150821e862f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-c476e98ae172>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'generator' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c476e98ae172>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ×˜×¢×™× ×ª ×”×ž×©×§×œ×™× ×œ×ž×•×“×œ×™× (× × ×™×— generator ×•-discriminator ×›×‘×¨ ×§×™×™×ž×™×)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'discriminator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"]}]},{"cell_type":"markdown","source":["This script provides two main functionalities:\n","\n","Sequence Generation:\n","\n","- The generate_sequence function takes a seed token sequence and uses the trained generator model to extend it until it reaches a target length.\n","- It uses temperature-controlled multinomial sampling to pick the next token from the generatorâ€™s output, ensuring variability in the generated sequence.\n","\n","Token-to-Music Conversion with music21:\n","- A pitch class mapping converts numerical values to note names.\n","- The parse_duration function converts duration tokens (in 16th-note units) to standard quarter lengths.\n","- The token_to_music21 function converts individual tokens into music21 objects (notes, chords, or rests) based on their prefix.\n","- The build_music21_stream function assembles a full music21 stream from a token sequence by first extracting context tokens (tempo, key, time signature) and then appending the musical events, enabling playback or further analysis."],"metadata":{"id":"AUcM-kXUZDfb"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import re\n","import music21\n","from music21 import stream, note, chord, tempo, key, meter\n","\n","# =============================================================================\n","# Function to generate a long sequence using probabilistic sampling\n","# =============================================================================\n","def generate_sequence(generator, seed_sequence, target_length, temperature=1.0):\n","    \"\"\"\n","    Receives a seed_sequence (a list of indices) and continues to generate tokens until the sequence reaches target_length.\n","    The temperature parameter affects the diversity of the choices.\n","    \"\"\"\n","    generator.eval()\n","    generated = seed_sequence.copy()\n","    while len(generated) < target_length:\n","        # Select the last window of input of length sequence_length\n","        current_window = generated[-sequence_length:]\n","        input_seq = torch.tensor([current_window], dtype=torch.long).to(device)\n","        with torch.no_grad():\n","            logits = generator(input_seq)  # Shape: [1, sequence_length, n_vocab]\n","        logits_last = logits[0, -1, :] / temperature\n","        probabilities = torch.softmax(logits_last, dim=-1)\n","        next_token = torch.multinomial(probabilities, num_samples=1).item()\n","        generated.append(next_token)\n","    return generated\n","\n","# =============================================================================\n","# Functions to convert tokens to music using music21\n","# =============================================================================\n","\n","# Definition of the pitch class mapping\n","pitch_class_map = {\n","    0: \"C\", 1: \"C#\", 2: \"D\", 3: \"D#\",\n","    4: \"E\", 5: \"F\", 6: \"F#\", 7: \"G\",\n","    8: \"G#\", 9: \"A\", 10: \"A#\", 11: \"B\"\n","}\n","\n","def parse_duration(dur_str, scale=1.0):\n","    \"\"\"\n","    The duration is given as a number of 16th note units; divide by 4 to get quarterLength.\n","    For example, '8' -> 8/4 = 2.0 quarterLength.\n","    With scale=1.0, the duration of each event remains as is.\n","    \"\"\"\n","    try:\n","        dur = float(dur_str)\n","    except:\n","        dur = 1.0\n","    return (dur / 4.0) * scale\n","\n","def token_to_music21(token):\n","    \"\"\"\n","    Converts a single token to a music21 object:\n","      - NOTE_<pitch>_<dur>: creates a note.Note object\n","      - CHORD_<chord_str>_<dur>: creates a chord.Chord object\n","      - REST_<dur>: creates a note.Rest object\n","    \"\"\"\n","    if token.startswith(\"NOTE_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 3:\n","            return None\n","        pitch_str = parts[1]\n","        dur = parse_duration(parts[2], scale=1.0)\n","        n_obj = note.Note(pitch_str)\n","        n_obj.quarterLength = dur\n","        return n_obj\n","    elif token.startswith(\"CHORD_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 3:\n","            return None\n","        chord_numbers_str = parts[1]\n","        dur = parse_duration(parts[2], scale=1.0)\n","        numbers = chord_numbers_str.split(\".\")\n","        pitches = []\n","        for num_str in numbers:\n","            try:\n","                num = int(num_str)\n","                pc = num % 12\n","                pitch_name = pitch_class_map.get(pc, \"C\")\n","                pitches.append(f\"{pitch_name}4\")\n","            except:\n","                continue\n","        if pitches:\n","            c_obj = chord.Chord(pitches)\n","            c_obj.quarterLength = dur\n","            return c_obj\n","        else:\n","            return None\n","    elif token.startswith(\"REST_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 2:\n","            return None\n","        dur = parse_duration(parts[1], scale=1.0)\n","        r_obj = note.Rest()\n","        r_obj.quarterLength = dur\n","        return r_obj\n","    else:\n","        return None\n","\n","def build_music21_stream(token_sequence):\n","    \"\"\"\n","    Receives a list of tokens (a sequence) and converts it to a music21 stream (music21.stream.Stream).\n","    Assumes that the first 3 tokens provide context: TEMPO, KEY, and TIME.\n","    \"\"\"\n","    s = stream.Stream()\n","\n","    if len(token_sequence) >= 3:\n","        tempo_token = token_sequence[0]  # e.g., TEMPO_120.0\n","        key_token = token_sequence[1]    # e.g., KEY_F#_major\n","        time_token = token_sequence[2]   # e.g., TIME_4_4\n","\n","        # Process tempo\n","        m = re.match(r\"TEMPO_(\\d+(\\.\\d+)?)\", tempo_token)\n","        if m:\n","            t = tempo.MetronomeMark(number=float(m.group(1)))\n","            s.insert(0, t)\n","\n","        # Process key: split by \"_\" and convert to format like \"F# major\"\n","        m = re.match(r\"KEY_(.+)\", key_token)\n","        if m:\n","            # First, replace \"_\" with space\n","            k_str = m.group(1).replace(\"_\", \" \").strip()\n","            try:\n","                # Try to create the Key object directly\n","                k_obj = key.Key(k_str)\n","                s.insert(0, k_obj)\n","            except Exception as e:\n","                # In case of error, try replacing \" sharp\" back to \"#\"\n","                k_str_fixed = k_str.replace(\" sharp\", \"#\")\n","                try:\n","                    k_obj = key.Key(k_str_fixed)\n","                    s.insert(0, k_obj)\n","                except Exception as e2:\n","                    print(\"Error processing key:\", k_str, e2)\n","\n","        # Process time signature\n","        m = re.match(r\"TIME_(.+)\", time_token)\n","        if m:\n","            ts_str = m.group(1).replace(\"_\", \"/\")\n","            try:\n","                ts_obj = meter.TimeSignature(ts_str)\n","                s.insert(0, ts_obj)\n","            except Exception as e:\n","                print(\"Error processing time signature:\", e)\n","\n","        events = token_sequence[3:]\n","    else:\n","        events = token_sequence\n","\n","    for token in events:\n","        m21_event = token_to_music21(token)\n","        if m21_event is not None:\n","            s.append(m21_event)\n","\n","    return s\n"],"metadata":{"id":"V4CPt8HKZFtM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This example demonstrates the end-to-end process of generating a new musical sequence and converting it to a MIDI file. Starting with a seed sequence from the dataset, the code uses the trained generator model to extend the sequence to a specified target length (e.g., 900 tokens) using temperature-controlled sampling. The generated token indices are then mapped back to their corresponding token strings. Finally, these tokens are converted into a music21 streamâ€”incorporating tempo, key, and time signature contextâ€”and written out as a MIDI file for playback or further analysis."],"metadata":{"id":"cKLRg2FeZzMa"}},{"cell_type":"code","source":["# =============================================================================\n","# Example usage:\n","# Assume that train_input, int_to_note, device, n_vocab, and sequence_length are already defined.\n","# Select a seed from the dataset (for example, the first sequence)\n","seed_sequence = train_input[0].tolist()\n","\n","# Define a higher target_length â€“ for instance, 1500 tokens to obtain a composition lasting about 1:30 minutes\n","target_length = 900\n","\n","# Generate the sequence\n","generated_sequence = generate_sequence(generator, seed_sequence, target_length, temperature=1.0)\n","generated_tokens = [int_to_note[token] for token in generated_sequence]\n","\n","print(\"Generated sequence length:\", len(generated_tokens))\n","print(\"First 50 tokens:\", generated_tokens[:50])\n","\n","# Convert the sequence to a music stream and generate a MIDI file\n","output_midi_path = '/content/drive/My Drive/generated_music.mid'\n","music_stream = build_music21_stream(generated_tokens)\n","music_stream.write('midi', fp=output_midi_path)\n","print(\"âœ… MIDI file generated successfully:\", output_midi_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dklTGCtngUbX","executionInfo":{"status":"ok","timestamp":1739789447263,"user_tz":-120,"elapsed":1775,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"d8c9ce75-b43d-4460-b216-674de11e443e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated sequence length: 900\n","First 50 tokens: ['TEMPO_120.0', 'KEY_F#_major', 'TIME_4_4', 'REST_8', 'REST_8', 'REST_10', 'CHORD_1.6_5', 'NOTE_B-4_8', 'NOTE_B-3_1', 'NOTE_C#4_1', 'CHORD_1.6_1', 'CHORD_10.1_1', 'REST_3', 'REST_0', 'CHORD_10.1_1', 'CHORD_1.6_2', 'REST_4', 'REST_8', 'REST_14', 'NOTE_B-4_1', 'NOTE_B-3_1', 'CHORD_6.10.1_3', 'CHORD_10.1_1', 'NOTE_F#4_3', 'REST_1', 'CHORD_10.1_1', 'CHORD_1.6_1', 'NOTE_B-4_3', 'REST_3', 'CHORD_10.1_1', 'NOTE_F#4_4', 'REST_3', 'NOTE_C#4_1', 'NOTE_B-3_1', 'NOTE_E-3_3', 'NOTE_E-2_3', 'NOTE_B-4_2', 'REST_0', 'CHORD_10.3_1', 'REST_1', 'REST_9', 'NOTE_E-3_6', 'NOTE_E-2_8', 'NOTE_B-4_5', 'CHORD_3.6_1', 'CHORD_10.3_1', 'REST_1', 'CHORD_10.3_1', 'NOTE_F#4_1', 'CHORD_10.3_1']\n","Error processing key: F# major # ajor is not a supported accidental type\n","âœ… MIDI file generated successfully: /content/drive/My Drive/generated_music.mid\n"]}]},{"cell_type":"markdown","source":["OUTPUT READY: PROCEEDING WITH TRAINING IMPROVEMENTS\n","**bold text**\n","\n","This snippet loads an existing checkpoint to resume training if needed. It restores the generator and discriminator weights, along with their optimizer states, ensuring that training can continue seamlessly from the saved epoch without losing progress"],"metadata":{"id":"DYQNqiTHijvT"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","from tqdm import tqdm\n","import pickle\n","\n","# -------------------------------\n","# Basic configurations and data loading\n","# -------------------------------\n","save_path = \"/content/drive/My Drive/checkpoints/\"\n","checkpoint_path = os.path.join(save_path, \"best_model_Gloss_0.0408.pth\")\n","\n","# Load the training data (if not already loaded)\n","with open('/content/drive/My Drive/training_input.pkl', 'rb') as f:\n","    train_inputs = pickle.load(f)\n","with open('/content/drive/My Drive/training_target.pkl', 'rb') as f:\n","    train_targets = pickle.load(f)\n","\n","print(\"Training inputs shape:\", train_inputs.shape)\n","print(\"Training targets shape:\", train_targets.shape)\n","\n","# Create DataLoader\n","dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.long),\n","                        torch.tensor(train_targets, dtype=torch.long))\n","batch_size = 512\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# -------------------------------\n","# Loading models and weights\n","# -------------------------------\n","# Assume that the models generator and discriminator have already been defined\n","# (SmallMusicGenerator, SmallMusicDiscriminator) and that note_to_int has been loaded,\n","# so we calculate n_vocab:\n","with open('/content/drive/My Drive/note_to_int.pkl', 'rb') as f:\n","    note_to_int = pickle.load(f)\n","n_vocab = len(note_to_int)\n","sequence_length = train_inputs.shape[1]  # for example, 200\n","\n","# Initialize the models (if not already initialized)\n","generator = SmallMusicGenerator(n_vocab=n_vocab, sequence_length=sequence_length)\n","discriminator = SmallMusicDiscriminator(n_vocab=n_vocab, sequence_length=sequence_length)\n","\n","# Move models to the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","generator.positional_encoding = generator.positional_encoding.to(device)\n","\n","# Load weights from the checkpoint (epoch 18)\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","generator.load_state_dict(checkpoint['generator_state_dict'])\n","discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","# Load optimizer states\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001)\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n","optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n","optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n","\n","# Set models to training mode\n","generator.train()\n","discriminator.train()\n","\n","print(f\"âœ… Checkpoint loaded from {checkpoint_path}. Continuing training from this point...\")\n","\n","# -------------------------------\n","# Define loss functions and additional optimizers, and set up AMP\n","# -------------------------------\n","bce_loss = nn.BCEWithLogitsLoss()   # for the discriminator\n","ce_loss = nn.CrossEntropyLoss()     # for Teacher Forcing\n","scaler = GradScaler()\n","\n","best_g_loss = float('inf')\n","start_epoch = 18        # assume the checkpoint is from epoch 18\n","epochs_to_run = 50      # for example, continue for another 50 epochs\n","\n","# -------------------------------\n","# Training Loop - Continue training from the checkpoint\n","# -------------------------------\n","for epoch in range(start_epoch, start_epoch + epochs_to_run):\n","    generator.train()\n","    discriminator.train()\n","\n","    epoch_g_loss = 0.0\n","    epoch_d_loss = 0.0\n","\n","    for batch_input, batch_target in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{start_epoch+epochs_to_run}\"):\n","        batch_input = batch_input.to(device).long()\n","        batch_target = batch_target.to(device).long()\n","\n","        ## === Update Discriminator === ##\n","        optimizer_D.zero_grad()\n","        real_labels = torch.ones(batch_input.shape[0], 1).to(device)\n","        fake_labels = torch.zeros(batch_input.shape[0], 1).to(device)\n","\n","        with autocast():\n","            # For real data: we pass batch_target (which is a sequence of indices)\n","            real_validity = discriminator(batch_target)  # [B, 1] (raw logits, no Sigmoid because using BCEWithLogitsLoss)\n","            loss_real = bce_loss(real_validity, real_labels)\n","\n","            # For fake data:\n","            fake_logits = generator(batch_input)  # [B, sequence_length, n_vocab]\n","            fake_tokens = torch.argmax(fake_logits, dim=-1).long()  # [B, sequence_length]\n","            fake_validity = discriminator(fake_tokens.detach())\n","            loss_fake = bce_loss(fake_validity, fake_labels)\n","\n","            d_loss = (loss_real + loss_fake) / 2.0\n","\n","        scaler.scale(d_loss).backward()\n","        scaler.step(optimizer_D)\n","        scaler.update()\n","\n","        ## === Update Generator === ##\n","        optimizer_G.zero_grad()\n","        with autocast():\n","            fake_logits = generator(batch_input)\n","            g_teacher_loss = ce_loss(fake_logits.view(-1, n_vocab), batch_target.view(-1))\n","\n","            fake_tokens = torch.argmax(fake_logits, dim=-1).long()\n","            validity = discriminator(fake_tokens.detach())\n","            g_adv_loss = bce_loss(validity, real_labels)\n","\n","            g_loss = g_teacher_loss + 0.002 * g_adv_loss\n","\n","        scaler.scale(g_loss).backward()\n","        scaler.step(optimizer_G)\n","        scaler.update()\n","\n","        epoch_g_loss += g_loss.item()\n","        epoch_d_loss += d_loss.item()\n","\n","    avg_g_loss = epoch_g_loss / len(data_loader)\n","    avg_d_loss = epoch_d_loss / len(data_loader)\n","    print(f\"Epoch {epoch+1}/{start_epoch+epochs_to_run}, Generator Loss: {avg_g_loss:.4f}, Discriminator Loss: {avg_d_loss:.4f}\")\n","\n","    if avg_g_loss < best_g_loss:\n","        best_g_loss = avg_g_loss\n","        best_save_file = os.path.join(save_path, f\"best_model_Gloss_{avg_g_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, best_save_file)\n","        print(f\"âœ… Best model saved at epoch {epoch+1} (Best Generator Loss: {best_g_loss:.4f})\")\n","\n","    if (epoch + 1) % 5 == 0:\n","        save_file = os.path.join(save_path, f\"epoch_{epoch+1}_Gloss_{avg_g_loss:.4f}_Dloss_{avg_d_loss:.4f}.pth\")\n","        torch.save({\n","            'generator_state_dict': generator.state_dict(),\n","            'discriminator_state_dict': discriminator.state_dict(),\n","            'optimizer_G_state_dict': optimizer_G.state_dict(),\n","            'optimizer_D_state_dict': optimizer_D.state_dict(),\n","        }, save_file)\n","        print(f\"ðŸ’¾ Saved model at epoch {epoch+1}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"9s-mWUBiip1p","executionInfo":{"status":"error","timestamp":1739888028486,"user_tz":-120,"elapsed":387,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"4ea2488e-737f-4e9c-a261-fab2f887d01d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/training_input.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fd862ee10f76>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ×˜×¢×Ÿ ××ª ×ž×¢×¨×š ×”××™×ž×•×Ÿ (×× ×›×‘×¨ ×œ× × ×˜×¢×Ÿ)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/training_input.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/training_target.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/training_input.pkl'"]}]},{"cell_type":"markdown","source":["load weight and eval"],"metadata":{"id":"52QDajPmxYmG"}},{"cell_type":"code","source":["import torch\n","import pickle\n","from tqdm import tqdm\n","import os\n","import numpy as np\n","\n","# Set device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the mappings (note_to_int and int_to_note) â€“ ensure these files exist\n","with open('/content/drive/My Drive/note_to_int.pkl', 'rb') as f:\n","    note_to_int = pickle.load(f)\n","with open('/content/drive/My Drive/int_to_note.pkl', 'rb') as f:\n","    int_to_note = pickle.load(f)\n","n_vocab = len(note_to_int)\n","\n","# Set sequence length (for example, 200 tokens)\n","sequence_length = 200\n","# Initialize the models\n","generator = SmallMusicGenerator(n_vocab=n_vocab, sequence_length=sequence_length)\n","discriminator = SmallMusicDiscriminator(n_vocab=n_vocab, sequence_length=sequence_length)\n","\n","# Move models to the appropriate device\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)\n","generator.positional_encoding = generator.positional_encoding.to(device)\n","\n","# Load the checkpoint (using weights_only=True to avoid FutureWarning)\n","checkpoint_path = \"/content/drive/My Drive/checkpoints/best_model_Gloss_0.0319.pth\"  # update this path accordingly\n","checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n","generator.load_state_dict(checkpoint['generator_state_dict'])\n","discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","print(\"âœ… Checkpoint loaded successfully.\")\n","\n","# Set models to evaluation mode\n","generator.eval()\n","discriminator.eval()\n","\n","# Load the training data (choose a random example for evaluation)\n","with open('/content/drive/My Drive/training_target.pkl', 'rb') as f:\n","    train_inputs = pickle.load(f)\n","\n","print(\"Training inputs shape:\", train_inputs.shape)\n","\n","# Select a random example\n","sample_idx = np.random.randint(0, train_inputs.shape[0])\n","sample_input = torch.tensor(train_inputs[sample_idx:sample_idx+1], dtype=torch.long).to(device)\n","\n","# Generate output from the generator in evaluation mode\n","with torch.no_grad():\n","    output_logits = generator(sample_input)  # Output shape: [1, sequence_length, n_vocab]\n","    predicted_indices = torch.argmax(output_logits, dim=-1)  # [1, sequence_length]\n","\n","# Convert indices to tokens\n","predicted_tokens = [int_to_note[idx.item()] for idx in predicted_indices[0]]\n","print(\"Predicted sequence:\")\n","print(predicted_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jB7a4sc7xrvD","executionInfo":{"status":"ok","timestamp":1740407231571,"user_tz":-120,"elapsed":18969,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"cd61c50d-d048-465a-ec2b-f6e29a60de18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Checkpoint loaded successfully.\n","Training inputs shape: (3510447, 200)\n","Predicted sequence:\n","['CHORD_10.11.1_2', 'REST_1', 'CHORD_8.11.1_1', 'REST_1', 'CHORD_5.8.11.1_1', 'REST_2', 'REST_12', 'CHORD_5.8.11.1_1', 'REST_2', 'CHORD_6.10.1_1', 'REST_1', 'NOTE_G1_1', 'REST_2', 'CHORD_10.11.1.5_1', 'NOTE_B-1_1', 'REST_1', 'NOTE_F2_1', 'REST_3', 'REST_1', 'REST_1', 'REST_7', 'REST_14', 'REST_14', 'CHORD_3.6.10_3', 'REST_3', 'CHORD_5.8.0_4', 'CHORD_5_1', 'REST_1', 'NOTE_G#1_1', 'NOTE_G#2_1', 'REST_3', 'REST_2', 'NOTE_E-5_2', 'CHORD_3.8_2', 'NOTE_C#5_2', 'CHORD_1.3_2', 'REST_7', 'REST_9', 'REST_10', 'REST_10', 'REST_10', 'NOTE_E-5_9', 'CHORD_3.8_4', 'NOTE_C#5_7', 'CHORD_1.3_4', 'CHORD_8_1', 'REST_1', 'CHORD_3_2', 'CHORD_8.0_7', 'CHORD_3.8_4', 'NOTE_C4_3', 'NOTE_G3_1', 'REST_5', 'REST_4', 'REST_3', 'REST_2', 'CHORD_5_1', 'REST_1', 'REST_5', 'REST_5', 'REST_11', 'CHORD_5_2', 'CHORD_8.0_2', 'CHORD_1_1', 'REST_3', 'NOTE_F3_3', 'CHORD_1.5.8_6', 'CHORD_1.5_6', 'REST_3', 'NOTE_E-3_3', 'NOTE_E-2_5', 'REST_5', 'REST_5', 'REST_3', 'CHORD_1.5_5', 'CHORD_8.1_3', 'NOTE_C#4_1', 'REST_11', 'NOTE_E-2_1', 'REST_10', 'REST_2', 'CHORD_6.8.0_2', 'NOTE_E-4_11', 'REST_4', 'NOTE_G#2_5', 'CHORD_5.8_1', 'NOTE_C#4_5', 'REST_3', 'NOTE_F#2_1', 'NOTE_G#3_1', 'NOTE_F#3_3', 'REST_5', 'NOTE_F#2_5', 'NOTE_C#4_1', 'REST_4', 'REST_2', 'NOTE_G#4_7', 'CHORD_5.8_2', 'NOTE_C#4_1', 'REST_9', 'REST_3', 'CHORD_6.10.1_3', 'REST_3', 'REST_2', 'CHORD_5.10_1', 'CHORD_10.1.5_1', 'REST_4', 'REST_5', 'REST_10', 'CHORD_5.10_4', 'CHORD_10.1.5_1', 'CHORD_8.10.1_1', 'NOTE_G#2_6', 'REST_5', 'CHORD_6.10_2', 'NOTE_C#4_4', 'REST_4', 'REST_3', 'REST_2', 'NOTE_G#4_1', 'CHORD_1.5.8_1', 'REST_4', 'REST_4', 'REST_4', 'NOTE_G#4_7', 'CHORD_1.5.8_1', 'CHORD_6.10_3', 'CHORD_1_4', 'NOTE_C#2_6', 'REST_3', 'REST_7', 'CHORD_1.5.8_1', 'REST_5', 'REST_0', 'CHORD_3_1', 'REST_1', 'CHORD_5_1', 'REST_0', 'NOTE_C#4_1', 'NOTE_E-3_1', 'CHORD_8.10.3_1', 'REST_4', 'REST_5', 'REST_14', 'NOTE_C#4_7', 'CHORD_8.10.3_1', 'CHORD_3.7.10_3', 'NOTE_E-4_2', 'NOTE_F4_2', 'REST_2', 'CHORD_8.0_1', 'NOTE_E-4_7', 'REST_3', 'CHORD_8.10_1', 'CHORD_3.8_1', 'CHORD_0_1', 'REST_1', 'REST_1', 'CHORD_8_1', 'REST_3', 'REST_2', 'CHORD_8.0.3_3', 'CHORD_8_7', 'REST_1', 'CHORD_1.4.8_1', 'CHORD_3.6.8_1', 'REST_1', 'REST_6', 'CHORD_1.4.8_1', 'REST_2', 'CHORD_6.8.9.0_2', 'CHORD_0.4.8_1', 'CHORD_8_1', 'REST_10', 'CHORD_3.6.8_1', 'REST_0', 'CHORD_8.9.0.3_1', 'REST_1', 'CHORD_8_1', 'REST_0', 'CHORD_1.4.7.9_2', 'REST_1', 'CHORD_9.1_1', 'NOTE_E-5_3', 'CHORD_4.9_1', 'REST_1', 'NOTE_B3_1', 'REST_0', 'CHORD_6.9.1_1', 'REST_1', 'REST_1', 'CHORD_9.1_1', 'NOTE_A3_1', 'REST_7', 'CHORD_9.1_0', 'REST_1', 'CHORD_7.9.1_1', 'CHORD_7_1', 'CHORD_1.4.7.9_1', 'REST_1']\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import re\n","import music21\n","from music21 import stream, note, chord, tempo, key, meter\n","import random\n","\n","# =============================================================================\n","# Function to generate a long sequence using probabilistic sampling\n","# =============================================================================\n","def generate_sequence(generator, seed_sequence, target_length, temperature=1.0):\n","    \"\"\"\n","    Receives a seed_sequence (a list of indices) and continues to generate tokens until the sequence reaches target_length.\n","    The temperature parameter influences the diversity of the selection.\n","    \"\"\"\n","    generator.eval()\n","    generated = seed_sequence.copy()\n","    while len(generated) < target_length:\n","        # Select the last input window of length sequence_length\n","        current_window = generated[-sequence_length:]\n","        input_seq = torch.tensor([current_window], dtype=torch.long).to(device)\n","        with torch.no_grad():\n","            logits = generator(input_seq)  # Shape: [1, sequence_length, n_vocab]\n","        logits_last = logits[0, -1, :] / temperature\n","        probabilities = torch.softmax(logits_last, dim=-1)\n","        next_token = torch.multinomial(probabilities, num_samples=1).item()\n","        generated.append(next_token)\n","    return generated\n","\n","# =============================================================================\n","# Functions to convert tokens to music using music21\n","# =============================================================================\n","\n","# Define pitch class mapping\n","pitch_class_map = {\n","    0: \"C\", 1: \"C#\", 2: \"D\", 3: \"D#\",\n","    4: \"E\", 5: \"F\", 6: \"F#\", 7: \"G\",\n","    8: \"G#\", 9: \"A\", 10: \"A#\", 11: \"B\"\n","}\n","\n","def parse_duration(dur_str, scale=1.0):\n","    \"\"\"\n","    The duration is given as a number of 16th units; we divide by 4 to get quarterLength.\n","    For example, '8' â†’ 8/4 = 2.0 quarterLength.\n","    With scale=1.0 â€“ meaning the duration of each event remains as is.\n","    \"\"\"\n","    try:\n","        dur = float(dur_str)\n","    except:\n","        dur = 1.0\n","    return (dur / 4.0) * scale\n","\n","def token_to_music21(token):\n","    \"\"\"\n","    Converts a single token to a music21 object:\n","      - NOTE_<pitch>_<dur>: creates a note.Note object\n","      - CHORD_<chord_str>_<dur>: creates a chord.Chord object\n","      - REST_<dur>: creates a note.Rest object\n","    \"\"\"\n","    if token.startswith(\"NOTE_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 3:\n","            return None\n","        pitch_str = parts[1]\n","        dur = parse_duration(parts[2], scale=1.0)\n","        n_obj = note.Note(pitch_str)\n","        n_obj.quarterLength = dur\n","        return n_obj\n","    elif token.startswith(\"CHORD_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 3:\n","            return None\n","        chord_numbers_str = parts[1]\n","        dur = parse_duration(parts[2], scale=1.0)\n","        numbers = chord_numbers_str.split(\".\")\n","        pitches = []\n","        for num_str in numbers:\n","            try:\n","                num = int(num_str)\n","                pc = num % 12\n","                pitch_name = pitch_class_map.get(pc, \"C\")\n","                pitches.append(f\"{pitch_name}4\")\n","            except:\n","                continue\n","        if pitches:\n","            c_obj = chord.Chord(pitches)\n","            c_obj.quarterLength = dur\n","            return c_obj\n","        else:\n","            return None\n","    elif token.startswith(\"REST_\"):\n","        parts = token.split(\"_\")\n","        if len(parts) < 2:\n","            return None\n","        dur = parse_duration(parts[1], scale=1.0)\n","        r_obj = note.Rest()\n","        r_obj.quarterLength = dur\n","        return r_obj\n","    else:\n","        return None\n","\n","def build_music21_stream(token_sequence):\n","    \"\"\"\n","    Receives a list of tokens (a sequence) and converts it into a music21 stream (music21.stream.Stream).\n","    Assumes that the first 3 tokens provide context: TEMPO, KEY, and TIME.\n","    \"\"\"\n","    s = stream.Stream()\n","\n","    if len(token_sequence) >= 3:\n","        tempo_token = token_sequence[0]  # e.g., TEMPO_120.0\n","        key_token = token_sequence[1]    # e.g., KEY_F#_major\n","        time_token = token_sequence[2]   # e.g., TIME_4_4\n","\n","        # Process tempo\n","        m = re.match(r\"TEMPO_(\\d+(\\.\\d+)?)\", tempo_token)\n","        if m:\n","            t = tempo.MetronomeMark(number=float(m.group(1)))\n","            s.insert(0, t)\n","\n","        # Process key: split by \"_\" and convert so the string is in the format \"F# major\"\n","        m = re.match(r\"KEY_(.+)\", key_token)\n","        if m:\n","            # First, replace \"_\" with a space\n","            k_str = m.group(1).replace(\"_\", \" \").strip()\n","            try:\n","                # Try to create the Key object directly\n","                k_obj = key.Key(k_str)\n","                s.insert(0, k_obj)\n","            except Exception as e:\n","                # In case of an error, try replacing \" sharp\" back to \"#\"\n","                k_str_fixed = k_str.replace(\" sharp\", \"#\")\n","                try:\n","                    k_obj = key.Key(k_str_fixed)\n","                    s.insert(0, k_obj)\n","                except Exception as e2:\n","                    print(\"Error processing key:\", k_str, e2)\n","\n","        # Process time signature\n","        m = re.match(r\"TIME_(.+)\", time_token)\n","        if m:\n","            ts_str = m.group(1).replace(\"_\", \"/\")\n","            try:\n","                ts_obj = meter.TimeSignature(ts_str)\n","                s.insert(0, ts_obj)\n","            except Exception as e:\n","                print(\"Error processing time signature:\", e)\n","\n","        events = token_sequence[3:]\n","    else:\n","        events = token_sequence\n","\n","    for token in events:\n","        m21_event = token_to_music21(token)\n","        if m21_event is not None:\n","            s.append(m21_event)\n","\n","    return s\n","\n","# =============================================================================\n","# Example usage:\n","# Assume that train_input, int_to_note, device, n_vocab, and sequence_length are already defined.\n","# Select a seed from the dataset (for example, the first sequence)\n","# seed_sequence = train_inputs[0].tolist()\n","seed_sequence = random.choice(train_inputs).tolist()\n","# Define a higher target_length â€“ for example, 1500 tokens to obtain a composition of about 1:30 minutes\n","target_length = 500\n","\n","# Generate the sequence\n","generated_sequence = generate_sequence(generator, seed_sequence, target_length, temperature=1.0)\n","generated_tokens = [int_to_note[token] for token in generated_sequence]\n","\n","print(\"Generated sequence length:\", len(generated_tokens))\n","print(\"First 50 tokens:\", generated_tokens[:50])\n","\n","# Convert the sequence to a music stream and generate a MIDI file\n","music_stream = build_music21_stream(generated_tokens)\n","output_midi_path = '/content/drive/My Drive/generated_music.mid'\n","music_stream.write('midi', fp=output_midi_path)\n","print(\"âœ… MIDI file generated successfully:\", output_midi_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNBOwdkx1sRb","executionInfo":{"status":"ok","timestamp":1740408232999,"user_tz":-120,"elapsed":2901,"user":{"displayName":"× ×™×‘ ×ž×œ×›×”","userId":"05530213554455520003"}},"outputId":"d4fc2b5e-7458-46c1-a9dd-38a5e0ca873d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated sequence length: 500\n","First 50 tokens: ['CHORD_2.4_1', 'REST_1', 'CHORD_10.2_1', 'NOTE_C#4_1', 'REST_0', 'REST_2', 'NOTE_E4_1', 'NOTE_D4_1', 'REST_3', 'REST_13', 'NOTE_B2_3', 'CHORD_11.1_1', 'REST_0', 'CHORD_11.1.2_1', 'REST_4', 'REST_1', 'NOTE_B4_3', 'NOTE_C#4_2', 'REST_1', 'NOTE_F#5_4', 'REST_2', 'NOTE_E-4_2', 'NOTE_A4_3', 'REST_1', 'REST_2', 'NOTE_E4_1', 'REST_1', 'REST_3', 'NOTE_E4_2', 'NOTE_A4_1', 'CHORD_9.11_1', 'REST_0', 'CHORD_8.9_1', 'CHORD_4.9_1', 'REST_1', 'REST_3', 'NOTE_B4_3', 'NOTE_F#3_2', 'REST_1', 'NOTE_D4_3', 'REST_2', 'NOTE_G#3_2', 'REST_1', 'REST_2', 'NOTE_B4_1', 'REST_0', 'NOTE_C#4_1', 'NOTE_A2_1', 'NOTE_A4_1', 'REST_2']\n","âœ… MIDI file generated successfully: /content/drive/My Drive/generated_music.mid\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HYQVQ2aJXDhA"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPsz8gKh8bPQMmmLgZssWSP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}